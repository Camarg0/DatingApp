{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Camarg0/DatingApp/blob/main/Aula02_tokenizacao_pratica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 2 - Tokenização"
      ],
      "metadata": {
        "id": "R8ElgxbDEATe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df51b504"
      },
      "source": [
        "game_names = [\n",
        "    \"Fable II\", \"Castlevania: Symphony of the Night\", \"Minecraft Story Mode\", \"Mega Man X\", \"The Walking Dead (Telltale)\", \"Super Mario Galaxy\", \"Destiny 2\", \"Team Fortress 2\", \"Cuphead\", \"Super Smash Bros. Melee\", \"Tomb Raider\", \"Ninja Gaiden Black\", \"Street Fighter IV\", \"World of Tanks\", \"Battlefront II (2017)\", \"SimCity 4\", \"Warcraft III: Reign of Chaos\", \"Mario Kart 8 Deluxe\", \"The Legend of Zelda: Breath of the Wild\", \"Donkey Kong Country\", \"Final Fantasy XIV\", \"The Secret of Monkey Island\", \"Halo: Combat Evolved\", \"Tales of Symphonia\", \"Horizon Zero Dawn\", \"The Witcher 2: Assassins of Kings\", \"Spelunky\", \"Starcraft II\", \"Darksiders\", \"Worms Armageddon\", \"Splatoon 2\", \"Scribblenauts\", \"Duck Hunt\", \"Spartan Total Warrior\", \"Command & Conquer: Red Alert\", \"Simpsons Hit & Run\", \"Lords of the Fallen\", \"Ratchet & Clank\", \"Manhunt\", \"The Crew 2\", \"Unravel\", \"Fahrenheit (Indigo Prophecy)\", \"Bioshock\", \"Dead Space\", \"Mega Man 2\", \"Kingdom Hearts II\", \"Frostpunk\", \"Nex Machina\", \"Mass Effect 2\", \"Elite Dangerous\", \"XCOM 2\", \"FIFA 21\", \"NHL 21\", \"Watch Dogs\", \"Fallout: New Vegas\",\n",
        "    \"The Legend of Zelda\", \"Super Mario Bros.\", \"Minecraft\", \"Fortnite\", \"League of Legends\", \"Call of Duty\", \"Grand Theft Auto V\", \"Red Dead Redemption 2\", \"The Witcher 3: Wild Hunt\", \"Overwatch\", \"PUBG\", \"Dark Souls\", \"Elden Ring\", \"Final Fantasy VII\", \"Metal Gear Solid\", \"Halo\", \"The Elder Scrolls V: Skyrim\", \"Apex Legends\", \"Counter-Strike: Global Offensive\", \"Tetris\", \"The Last of Us\", \"God of War\", \"Uncharted 4\", \"Resident Evil 4\", \"The Sims\", \"Animal Crossing: New Horizons\", \"Pokémon Red/Blue\", \"World of Warcraft\", \"Diablo III\", \"Street Fighter II\", \"Pac-Man\", \"Street Fighter V\", \"Mortal Kombat\", \"Kingdom Hearts\", \"Crash Bandicoot\", \"Spyro the Dragon\", \"Celeste\", \"Hollow Knight\", \"Undertale\", \"Bioshock Infinite\", \"Gears of War\", \"Fallout 4\", \"Rocket League\", \"Assassin's Creed II\", \"Bloodborne\", \"The Division 2\", \"FIFA 22\", \"NBA 2K22\", \"Super Smash Bros. Ultimate\", \"Dead by Daylight\", \"Hearthstone\", \"Half-Life 2\", \"Journey\", \"Resident Evil Village\", \"Dishonored\", \"Control\", \"Watch Dogs 2\", \"Divinity: Original Sin 2\", \"Shovel Knight\", \"Final Fantasy XV\", \"Battlefield V\", \"Nier: Automata\", \"The Legend of Heroes: Trails in the Sky\", \"Sekiro: Shadows Die Twice\", \"Doom (2016)\", \"Cyberpunk 2077\", \"Among Us\", \"Portal 2\", \"The Binding of Isaac\", \"No Man's Sky\", \"Left 4 Dead 2\", \"Minecraft Dungeons\", \"Guitar Hero III: Legends of Rock\", \"The Last of Us Part II\", \"Fall Guys: Ultimate Knockout\", \"Kingdom Come: Deliverance\", \"Super Metroid\", \"Shadow of the Colossus\", \"Super Mario Odyssey\", \"Pokémon Go\", \"SimCity\", \"L.A. Noire\", \"Grand Theft Auto: San Andreas\", \"NBA Jam\", \"Diablo II\", \"Braid\", \"Resident Evil 2 (Remake)\", \"Dragon Quest XI\", \"Super Mario 64\",\n",
        "    \"Grand Theft Auto VI\", \"Cyberpunk 2078\", \"The Legend of Zelda: Tears of the Kingdom\", \"Elden Ring 2\", \"God of War: Ragnarök\", \"Starfield\", \"Hogwarts Legacy\", \"Baldur's Gate 3\", \"Spider-Man 2\", \"Alan Wake 2\", \"Diablo IV\", \"Street Fighter 6\", \"Mortal Kombat 1\", \"Star Wars Jedi: Survivor\", \"Final Fantasy XVI\", \"Resident Evil 4 Remake\", \"Dead Space Remake\", \"Persona 5 Royal\", \"Octopath Traveler II\", \"Zelda: Breath of the Wild 2\", \"Metroid Prime 4\", \"Fable (reboot)\", \"The Witcher 4\", \"Mass Effect 4\", \"Dragon Age: Dreadwolf\", \"Avowed\", \"State of Decay 3\", \"Forza Motorsport (2023)\", \"Microsoft Flight Simulator 2024\", \"Hellblade II: Senua's Saga\", \"Perfect Dark (reboot)\", \"Everwild\", \"Indiana Jones and the Great Circle\", \"South of Midnight\", \"Clockwork Revolution\", \"Project: Mara\", \"The Outer Worlds 2\", \"Contraband\", \"Gears of War 6\", \"Age of Mythology: Retold\", \"Valorant\", \"Genshin Impact\", \"Roblox\", \"Fortnite\", \"Apex Legends\", \"League of Legends\", \"Dota 2\", \"Counter-Strike 2\", \"Overwatch 2\", \"Call of Duty: Modern Warfare III\", \"EA Sports FC 24\", \"NBA 2K24\", \"F1 23\", \"Madden NFL 24\", \"NHL 24\", \"Cyberpunk 2077: Phantom Liberty\", \"Horizon Forbidden West: Burning Shores\", \"Destiny 2: Lightfall\", \"The Sims 4: For Rent\", \"Minecraft: Trails & Tales\", \"World of Warcraft: Dragonflight\", \"Final Fantasy XIV: Dawntrail\", \"Elder Scrolls Online: Necrom\", \"Path of Exile 2\", \"Star Citizen\", \"Squadron 42\", \"Ark 2\", \"Palworld\", \"Helldivers 2\", \"Enshrouded\", \"Last Epoch\", \"No Rest for the Wicked\", \"Black Myth: Wukong\", \"Vampire: The Masquerade - Bloodlines 2\", \"S.T.A.L.K.E.R. 2: Heart of Chornobyl\", \"Warhammer 40,000: Space Marine 2\", \"Tekken 8\", \"Prince of Persia: The Lost Crown\", \"Persona 3 Reload\", \"Like a Dragon: Infinite Wealth\", \"Granblue Fantasy: Relink\", \"Unicorn Overlord\", \"Final Fantasy VII Rebirth\", \"Dragon's Dogma 2\", \"Rise of the Ronin\", \"Princess Peach: Showtime!\", \"Alone in the Dark (2024)\", \"Senua's Saga: Hellblade II\", \"Sand Land\", \"Eiyuden Chronicle: Hundred Heroes\", \"Paper Mario: The Thousand-Year Door (Switch)\", \"Luigi's Mansion 2 HD\", \"Monster Hunter Stories (Switch)\", \"Mixtape\", \"Silent Hill 2 Remake\", \"Death Stranding 2\", \"Marvel's Wolverine\", \"State of Play (PlayStation)\", \"Grounded\", \"Sea of Thieves\", \"Ori and the Will of the Wisps\", \"Psychonauts 2\", \"Hi-Fi Rush\",\n",
        "    \"Hades\", \"Disco Elysium\", \"Control\", \"Outer Wilds\", \"Deathloop\", \"Returnal\", \"It Takes Two\", \"Psychonauts 2\", \"Ratchet & Clank: Rift Apart\", \"Guardians of the Galaxy\", \"Forza Horizon 5\", \"Halo Infinite\", \"Metroid Dread\", \"Resident Evil Village\", \"Hitman 3\", \"Sifu\", \"Stray\", \"Cult of the Lamb\", \"Vampire Survivors\", \"Pentiment\", \"God of War Ragnarök\", \"Elden Ring\", \"Horizon Forbidden West\", \"Gran Turismo 7\", \"A Plague Tale: Requiem\", \"Bayonetta 3\", \"Splatoon 3\", \"Xenoblade Chronicles 3\", \"Kirby and the Forgotten Land\", \"Neon White\", \"Immortality\", \"Tunic\", \"Norco\", \"Rollerdrome\", \"Marvel Snap\", \"Citizen Sleeper\", \"Vampire: The Masquerade - Bloodhunt\", \"Chained Echoes\", \"SIGNALIS\", \"The Stanley Parable: Ultra Deluxe\", \"Hardspace: Shipbreaker\", \"PowerWash Simulator\", \"Cuphead: The Delicious Last Course\", \"Teenage Mutant Ninja Turtles: Shredder's Revenge\", \"Moss: Book II\", \"Synapse\", \"Redfall\", \"Star Wars Jedi: Survivor\", \"Wo Long: Fallen Dynasty\", \"Remnant II\", \"Armored Core 6: Fires of Rubicon\", \"Baldur's Gate 3\", \"Cyberpunk 2077: Phantom Liberty\", \"Alan Wake 2\", \"Marvel's Spider-Man 2\", \"Super Mario Bros. Wonder\", \"Pikmin 4\", \"Street Fighter 6\", \"Mortal Kombat 1\", \"Diablo IV\", \"Final Fantasy XVI\", \"Hogwarts Legacy\", \"Dead Space Remake\", \"Resident Evil 4 Remake\", \"Octopath Traveler II\", \"Persona 3 Reload\", \"Like a Dragon: Infinite Wealth\", \"Granblue Fantasy: Relink\", \"Unicorn Overlord\", \"Final Fantasy VII Rebirth\", \"Dragon's Dogma 2\", \"Rise of the Ronin\", \"Princess Peach: Showtime!\", \"Alone in the Dark (2024)\", \"Senua's Saga: Hellblade II\", \"Sand Land\", \"Eiyuden Chronicle: Hundred Heroes\", \"Paper Mario: The Thousand-Year Door (Switch)\", \"Luigi's Mansion 2 HD\", \"Monster Hunter Stories (Switch)\", \"Mixtape\", \"Silent Hill 2 Remake\", \"Death Stranding 2\", \"Marvel's Wolverine\", \"State of Play (PlayStation)\", \"Black Myth: Wukong\", \"S.T.A.L.K.E.R. 2: Heart of Chornobyl\", \"Warhammer 40,000: Space Marine 2\", \"Tekken 8\", \"Prince of Persia: The Lost Crown\", \"Palworld\", \"Helldivers 2\", \"Enshrouded\", \"Last Epoch\", \"No Rest for the Wicked\"\n",
        "]\n",
        "\n",
        "print(f\"Lista de jogos criada com {len(game_names)} títulos.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9da3411"
      },
      "source": [
        "# Recarregar o tokenizador treinado (se ainda não estiver carregado)\n",
        "from tokenizers import Tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# Exemplo de tokenização para alguns dos novos jogos adicionados\n",
        "# Selecionando jogos a partir do índice onde os novos títulos foram adicionados\n",
        "new_game_examples = game_names[245:250]\n",
        "\n",
        "print(\"\\nTokenização de exemplos de novos jogos:\")\n",
        "for texto in new_game_examples:\n",
        "    out = tokenizer.encode(texto)\n",
        "    print(f\"Texto: {texto}\")\n",
        "    print(f\"Tokens: {out.tokens}\")\n",
        "    print(f\"IDs: {out.ids}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Pré-tokenização\n"
      ],
      "metadata": {
        "id": "2ZRQpsLhIGNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers\n",
        "\n",
        "tok_ws = Tokenizer(models.BPE())\n",
        "tok_ws.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "# Usando um nome de jogo da lista para demonstração\n",
        "frase_exemplo = game_names[0] # Pegando o primeiro jogo da lista\n",
        "\n",
        "print(f\"Original: {frase_exemplo}\")\n",
        "print(tok_ws.pre_tokenizer.pre_tokenize_str(frase_exemplo))"
      ],
      "metadata": {
        "id": "HKHYbj5-FU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Punctuation + Whitespace*"
      ],
      "metadata": {
        "id": "XCj0m4asF14E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_punc = Tokenizer(models.BPE())\n",
        "tok_punc.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Punctuation()\n",
        "])\n",
        "# Usando um nome de jogo da lista para demonstração\n",
        "frase_exemplo = game_names[245] # Pegando o segundo jogo da lista para variar\n",
        "\n",
        "print(f\"Original: {frase_exemplo}\")\n",
        "print(tok_punc.pre_tokenizer.pre_tokenize_str(frase_exemplo))"
      ],
      "metadata": {
        "id": "GEhKhK5UFoZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretokenizer: ByteLevel - estilo GPT-2**"
      ],
      "metadata": {
        "id": "TE4jFk36Gw2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_byte = Tokenizer(models.BPE())\n",
        "tok_byte.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "# Usando um nome de jogo da lista para demonstração\n",
        "frase_exemplo = game_names[5] # Pegando o terceiro jogo da lista\n",
        "\n",
        "print(f\"Original: {frase_exemplo}\")\n",
        "print(tok_byte.pre_tokenizer.pre_tokenize_str(frase_exemplo))"
      ],
      "metadata": {
        "id": "Q9AANXjUF8wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metaspace (SentencePiece style)"
      ],
      "metadata": {
        "id": "MrgFHmqvG3tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_meta = Tokenizer(models.BPE())\n",
        "tok_meta.pre_tokenizer = pre_tokenizers.Metaspace()\n",
        "# Usando um nome de jogo da lista para demonstração\n",
        "frase_exemplo = game_names[234] # Pegando o quarto jogo da lista\n",
        "\n",
        "print(f\"Original: {frase_exemplo}\")\n",
        "print(tok_meta.pre_tokenizer.pre_tokenize_str(frase_exemplo))"
      ],
      "metadata": {
        "id": "h_5CuANPG55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento\n"
      ],
      "metadata": {
        "id": "XH9EDxlNH5PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 02_tokenizer_train.ipynb\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "# 1. Visão geral do algoritmo BPE\n",
        "print(\"Treinar o tokenizador (BPE):\\n\")\n",
        "print(\"1. Comece com todos os caracteres presentes no corpus como tokens.\")\n",
        "print(\"2. Encontre e una o par de tokens mais frequente em um novo token.\")\n",
        "print(\"3. Repita até atingir o tamanho de vocabulário desejado.\\n\")\n",
        "\n",
        "# 2. Corpus de treino - Usando a lista completa de game_names\n",
        "corpus = game_names\n",
        "print(f\"Corpus de treino com {len(corpus)} títulos.\\n\")\n",
        "\n",
        "# 3. Configuração do tokenizador\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "bpe_trainer = trainers.BpeTrainer(\n",
        "    vocab_size=3000, # Aumentando o vocabulário para acomodar mais palavras dos jogos\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
        ")\n",
        "\n",
        "# 4. Treinamento\n",
        "tokenizer.train_from_iterator(corpus, trainer=bpe_trainer)\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(f\"Tamanho do vocabulário: {len(vocab)}\\n\")\n",
        "\n",
        "# 5. Visualizando parte do vocabulário\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda kv: kv[1])[:20]\n",
        "for token, idx in sorted_vocab:\n",
        "    print(f\"{idx:>3} → {repr(token)}\")\n",
        "\n",
        "# 6. Salvando e recarregando\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "tokenizer_new = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# 7. Testando em novas frases - Usando os primeiros 5 jogos da lista\n",
        "textos = game_names[:5]\n",
        "\n",
        "print(\"\\nTokenização de exemplos:\")\n",
        "for texto in textos:\n",
        "    out = tokenizer_new.encode(texto)\n",
        "    print(f\"Texto: {texto}\")\n",
        "    print(f\"Tokens: {out.tokens}\")\n",
        "    print(f\"IDs: {out.ids}\\n\")"
      ],
      "metadata": {
        "id": "3KIL27TjILDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode"
      ],
      "metadata": {
        "id": "3bkj5aXXJ3xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_tokenizer_encode.ipynb\n",
        "# Pipeline de tokenização: normalização → pré-tokenização → modelo → pós-processamento\n",
        "\n",
        "from tokenizers import Tokenizer, normalizers, pre_tokenizers, processors\n",
        "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace, Digits, Sequence\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "print(\"### Pipeline de tokenização ###\")\n",
        "print(\" Normalization\")\n",
        "print(\" Pre-tokenization\")\n",
        "print(\" Model\")\n",
        "print(\" Post-processing\\n\")\n",
        "\n",
        "# Carregar o tokenizador treinado (BPE)\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Normalization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Normalization\")\n",
        "normalizer = normalizers.Sequence([\n",
        "    NFD(),          # decomposição de acentos\n",
        "    Lowercase(),    # tudo minúsculo\n",
        "    StripAccents()  # remove acentos\n",
        "])\n",
        "texto = game_names[0] # Usando o primeiro jogo da lista\n",
        "print(\"Antes:\", texto)\n",
        "print(\"Depois:\", normalizer.normalize_str(texto), \"\\n\")\n",
        "tokenizer.normalizer = normalizer\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Pre-tokenization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Pre-tokenization\")\n",
        "pre_tok = Sequence([\n",
        "    Whitespace(),\n",
        "    Digits(individual_digits=True)\n",
        "])\n",
        "texto2 = game_names[15] # Usando um jogo com números e pontuação se possível (ex: SimCity 4)\n",
        "print(\"Pré-tokenização:\", pre_tok.pre_tokenize_str(texto2), \"\\n\")\n",
        "tokenizer.pre_tokenizer = pre_tok\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Model\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Model: BPE (Byte Pair Encoding)\")\n",
        "# já carregado do arquivo bpe_tokenizer.json\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Post-processing\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Post-processing (TemplateProcessing)\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Aplicando tudo\n",
        "# -----------------------------------------------------------\n",
        "encoded = tokenizer.encode(game_names[0]) # Codificando o primeiro jogo da lista\n",
        "print(\"Tokens IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)"
      ],
      "metadata": {
        "id": "WLRBw0K_J1_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bytelevel vs SentencePiece"
      ],
      "metadata": {
        "id": "_XKY1lw2NuNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_bytelevel_vs_sentencepiece.ipynb\n",
        "# Comparando ByteLevel (GPT-2) vs SentencePiece (mT5)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import unicodedata\n",
        "\n",
        "# -----------------------------\n",
        "# 1️⃣ Modelos\n",
        "# -----------------------------\n",
        "BYTELEVEL_MODEL = \"openai-community/gpt2\"\n",
        "SENTPIECE_MODEL = \"google/mt5-small\"\n",
        "\n",
        "tok_byte = AutoTokenizer.from_pretrained(BYTELEVEL_MODEL)\n",
        "tok_spm  = AutoTokenizer.from_pretrained(SENTPIECE_MODEL)\n",
        "\n",
        "# Garantir pad_token\n",
        "if tok_byte.pad_token is None and hasattr(tok_byte, \"eos_token\"):\n",
        "    tok_byte.pad_token = tok_byte.eos_token\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Texto de exemplo - Usando um nome de jogo com pontuação/caracteres especiais\n",
        "# -----------------------------\n",
        "text = game_names[1] # \"Castlevania: Symphony of the Night\"\n",
        "print(f\"Texto: {text}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ Tokenização\n",
        "# -----------------------------\n",
        "def encode_details(tokenizer, name):\n",
        "    enc = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    ids = enc[\"input_ids\"]\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"IDs:\", ids)\n",
        "    print(\"Qtd tokens:\", len(tokens))\n",
        "    print(\"Decoded:\", tokenizer.decode(ids))\n",
        "    print(\"Offsets:\", offsets)\n",
        "    print()\n",
        "\n",
        "encode_details(tok_byte, \"ByteLevel (GPT-2)\")\n",
        "encode_details(tok_spm, \"SentencePiece (mT5)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Comparação Unicode (opcional)\n",
        "# -----------------------------\n",
        "def show_unicode_chars(s):\n",
        "    for ch in s:\n",
        "        name = unicodedata.name(ch, \"UNKNOWN\")\n",
        "        print(f\"{repr(ch)} -> {name}\")\n",
        "\n",
        "print(\"\\nCaracteres Unicode do texto:\")\n",
        "show_unicode_chars(text)"
      ],
      "metadata": {
        "id": "mhEG4EpoLiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação"
      ],
      "metadata": {
        "id": "cj23lfrwPLjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "import numpy as np\n",
        "import pandas as pd # Importar pandas aqui, se ainda não estiver importado no notebook\n",
        "\n",
        "# Carrega o tokenizador treinado\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# Corpus de teste - Usando a lista completa de game_names\n",
        "test_texts = game_names\n",
        "\n",
        "# Funções auxiliares\n",
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def evaluate_tokenizer(tokenizer, texts):\n",
        "    stats = []\n",
        "    for t in texts:\n",
        "        enc = tokenizer.encode(t)\n",
        "        # Adiciona uma verificação para evitar divisão por zero se não houver palavras/caracteres\n",
        "        num_words = count_words(t)\n",
        "        num_chars = count_chars(t)\n",
        "\n",
        "        stats.append({\n",
        "            \"text\": t,\n",
        "            \"chars\": num_chars,\n",
        "            \"words\": num_words,\n",
        "            \"tokens\": len(enc.tokens),\n",
        "            \"unk\": enc.tokens.count(\"<unk>\"),\n",
        "            \"decoded_ok\": (tokenizer.decode(enc.ids) == t)\n",
        "        })\n",
        "    return stats\n",
        "\n",
        "stats = evaluate_tokenizer(tokenizer, test_texts)\n",
        "\n",
        "# Converter para métricas agregadas\n",
        "df = pd.DataFrame(stats)\n",
        "\n",
        "# Calcula as métricas, adicionando verificação para evitar divisão por zero\n",
        "tpc = (df[\"tokens\"] / df[\"chars\"]).replace([np.inf, -np.inf], np.nan).mean() if not df[\"chars\"].eq(0).all() else np.nan\n",
        "tpw = (df[\"tokens\"] / df[\"words\"]).replace([np.inf, -np.inf], np.nan).mean() if not df[\"words\"].eq(0).all() else np.nan\n",
        "unk_rate = (df[\"unk\"].sum() / df[\"tokens\"].sum()) * 100 if df[\"tokens\"].sum() > 0 else 0\n",
        "decode_acc = (df[\"decoded_ok\"].mean()) * 100\n",
        "\n",
        "print(\"=== Métricas de eficiência ===\")\n",
        "print(f\"Tokens por caractere (TPC): {tpc:.3f}\")\n",
        "print(f\"Tokens por palavra (TPW): {tpw:.3f}\")\n",
        "print(f\"Percentual de <unk>: {unk_rate:.2f}%\")\n",
        "print(f\"Reversibilidade (decode == original): {decode_acc:.1f}%\")\n",
        "print(f\"Tamanho médio da sequência: {df['tokens'].mean():.1f} tokens/frase\")"
      ],
      "metadata": {
        "id": "hfiIxoW5PN6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}